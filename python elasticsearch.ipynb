{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(['localhost'],http_auth=('elastic', 'changeme'),port=9200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = {\n",
    "    'author': 'boto',\n",
    "    'text': 'boto love jazz',\n",
    "    'label':'negative',\n",
    "    'timestamp': datetime.now(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = es.index(index=\"test-python\", doc_type='news', id=3, body=doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def query_item(id):\n",
    "    res = es.get(index=\"test-python\", doc_type='news', id=id)\n",
    "    #print(res['_source'])\n",
    "    item=res['_source']\n",
    "    content=item['text']\n",
    "    sentiment=item['label']\n",
    "    return (content,sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bangkok is metropolitan of Thailand\n"
     ]
    }
   ],
   "source": [
    "aa=(query_item('2'))\n",
    "print(aa[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elasticsearch: cool. bonsai cool.\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "print(aa[0])\n",
    "print(aa[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#item=res['_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#content=item['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sentiment=item['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#result=res['_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def token_item(content):\n",
    "    ef=es.indices.analyze(text=content,analyzer=\"thai\")\n",
    "    gc=[]\n",
    "    for ge in ef['tokens']:\n",
    "        gc.append(ge['token'])\n",
    "    return gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fe=ef['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elasticsearch\n",
      "cool\n",
      "bonsai\n",
      "cool\n"
     ]
    }
   ],
   "source": [
    "for ge in ef['tokens']:\n",
    "    print(ge['token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc=[]\n",
    "for ge in ef['tokens']:\n",
    "    gc.append(ge['token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ga=(gc,sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['elasticsearch', 'cool', 'bonsai', 'cool'], 'positive')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tk=token_item(aa[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gg=(tk,aa[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['bangkok', 'is', 'metropolitan', 'of', 'thailand'], 'positive')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 3 Hits:\n",
      "2017-02-16T21:58:24.844689 jimbo: Bangkok is metropolitan of Thailand\n",
      "2017-02-16T20:49:56.885727 jimbo: Elasticsearch: cool. bonsai cool.\n",
      "2017-02-17T09:35:07.921608 boto: boto love jazz\n"
     ]
    }
   ],
   "source": [
    "res = es.search(index=\"test-python\", body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Got %d Hits:\" % res['hits']['total'])\n",
    "for hit in res['hits']['hits']:\n",
    "    print(\"%(timestamp)s %(author)s: %(text)s\" % hit[\"_source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 3 Hits:\n",
      "[(['bangkok', 'is', 'metropolitan', 'of', 'thailand'], 'positive'), (['elasticsearch', 'cool', 'bonsai', 'cool'], 'positive'), (['boto', 'love', 'jazz'], 'negative')]\n"
     ]
    }
   ],
   "source": [
    "res = es.search(index=\"test-python\", body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Got %d Hits:\" % res['hits']['total'])\n",
    "ox=[]\n",
    "for hit in res['hits']['hits']:\n",
    "    #print(hit['_id'])\n",
    "    #print(hit['_source']['author'])\n",
    "    aa=(query_item(hit['_id']))\n",
    "    tk=token_item(aa[0])\n",
    "    gg=(tk,aa[1])\n",
    "    ox.append(gg)\n",
    "\n",
    "print(ox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pythainlp.segment import segment\n",
    "from time import gmtime, strftime\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "      all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = nltk.classify.apply_features(extract_features, ox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_features = get_word_features(get_words_in_tweets(ox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love lampang : negative\n"
     ]
    }
   ],
   "source": [
    "#  ใส่ข้อความตรงอักษรสีแดง ด้านล่างนี้ #\n",
    "\n",
    "steep= 'I love lampang'\n",
    "\n",
    "# ระบุ Catagory  : Social ,Technology, Economic, Environment, Polotic #\n",
    "\n",
    "catatory = \"Environment\"\n",
    "\n",
    "\n",
    "steep=steep.replace(\",\",\"\")\n",
    "steepx=segment(steep)\n",
    "myString = \" \".join(steepx)\n",
    "\n",
    "answer=(classifier.classify(extract_features(myString.split())))\n",
    "print (steep+\" : \"+classifier.classify(extract_features(myString.split())))\n",
    "\n",
    "###\n",
    "#date=strftime(\"%Y-%m-%d\", gmtime())\n",
    "#text_file = codecs.open(\"D:/search/logstash-5.0.0/bin/industry4/Output.csv\", \"a\", 'UTF-8')\n",
    "#text_file.write(date+\",\"+catatory+\",\"+steep+\",\"+str(answer)+\"\\n\")\n",
    "#text_file.close()\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_max_id(index):\n",
    "    res = es.search(index=index, \n",
    "                            body={  \"stored_fields\": [ \"_uid\"],\n",
    "                                        \"query\": {\"match_all\": {}  },\n",
    "                                        \"size\": 1,\n",
    "                                        \"sort\": [ { \"_uid\" : { \"order\" : \"desc\"} } ]\n",
    "                                        })\n",
    "    return res['hits']['hits'][0][\"_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(get_max_id(\"test-python\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
